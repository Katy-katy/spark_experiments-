{
    "nbformat_minor": 1, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "file_extension": ".py", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }, 
            "nbconvert_exporter": "python", 
            "version": "2.7.11"
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark20", 
            "display_name": "Python 2 with Spark 2.0"
        }
    }, 
    "cells": [
        {
            "execution_count": 1, 
            "source": "sc", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "<pyspark.context.SparkContext at 0x7efddc687610>"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 2, 
            "source": "sc.version", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 2, 
                    "data": {
                        "text/plain": "u'2.0.2'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "The Data Science Experience notebook environment predefines the Spark context.\nIn other environments, you need to pick an interpreter (for example, pyspark for Python) and create a SparkConf object to initialize a SparkContext object. \n\n**For example: **\n\nfrom pyspark import SparkContext, SparkConf\n\nconf = SparkConf().setAppName(appName).setMaster(master)\n\nsc = SparkContext(conf=conf)", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 3, 
            "source": "x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] # a collection", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 4, 
            "source": "x_nbr_rdd = sc.parallelize(x) # RDD", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 5, 
            "source": "x_nbr_rdd.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 6, 
            "source": "x_nbr_rdd.take(5)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 6, 
                    "data": {
                        "text/plain": "[1, 2, 3, 4, 5]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 8, 
            "source": "y = [\"Hi everyone\", \"My Name is Kate\"]\ny_str_rdd = sc.parallelize(y)\ny_str_rdd.take(1)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 8, 
                    "data": {
                        "text/plain": "['Hi everyone']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 9, 
            "source": "x_nbr_rdd_2 = x_nbr_rdd.map(lambda x: x+1)\nx_nbr_rdd_2.collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 9, 
                    "data": {
                        "text/plain": "[2, 3, 4, 5, 6, 7, 8, 9, 10, 11]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 10, 
            "source": "X = [\"1,2,3,4,5,6,7,8,9,10\"] # we neeed \"\" to create an array, not a collection", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }, 
        {
            "execution_count": 11, 
            "source": "y_rd = sc.parallelize(X)\nSum_rd = y_rd.map(lambda y: y.split(\",\")).\\\nmap(lambda y: (int(y[2])+int(y[9])))\nSum_rd.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 11, 
                    "data": {
                        "text/plain": "13"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 12, 
            "source": "Words = [\"Hello Human. I'm Apache Spark and I love running analysis on data.\"]\nwords_rd = sc.parallelize(Words)\nwords_rd.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 12, 
                    "data": {
                        "text/plain": "\"Hello Human. I'm Apache Spark and I love running analysis on data.\""
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 13, 
            "source": "Words_rd2 = words_rd.map(lambda line: line.split(\" \"))\nWords_rd2.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 13, 
                    "data": {
                        "text/plain": "['Hello',\n 'Human.',\n \"I'm\",\n 'Apache',\n 'Spark',\n 'and',\n 'I',\n 'love',\n 'running',\n 'analysis',\n 'on',\n 'data.']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 14, 
            "source": "Words_rd2.count()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 14, 
                    "data": {
                        "text/plain": "1"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 15, 
            "source": "words_rd2 = words_rd.flatMap(lambda line: line.split(\" \"))\nwords_rd2.take(3)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 15, 
                    "data": {
                        "text/plain": "['Hello', 'Human.', \"I'm\"]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 16, 
            "source": "z = [\"First,Line\", \"Second,Line\", \"and,Third,Line\"]\nz_str_rdd = sc.parallelize(z)\nz_str_rdd.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 16, 
                    "data": {
                        "text/plain": "'First,Line'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 17, 
            "source": "z_str_rdd_split_flatmap = z_str_rdd.flatMap(lambda line: line.split(\",\"))\nz_str_rdd_split_flatmap.collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 17, 
                    "data": {
                        "text/plain": "['First', 'Line', 'Second', 'Line', 'and', 'Third', 'Line']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "### \"Map-Reduce\" in Spark", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 19, 
            "source": "countWords = z_str_rdd_split_flatmap.map(lambda word:(word,1))\ncountWords.collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 19, 
                    "data": {
                        "text/plain": "[('First', 1),\n ('Line', 1),\n ('Second', 1),\n ('Line', 1),\n ('and', 1),\n ('Third', 1),\n ('Line', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 20, 
            "source": "from operator import add\ncountWords2 = countWords.reduceByKey(add)\ncountWords2.collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 20, 
                    "data": {
                        "text/plain": "[('and', 1), ('Line', 3), ('Second', 1), ('Third', 1), ('First', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "source": "###  Filter", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 21, 
            "source": "words_rd3 = z_str_rdd_split_flatmap.filter(lambda line: \"Second\" in line) \n\nprint \"The count of words \" + str(words_rd3.first())\nprint \"Is: \" + str(words_rd3.count())", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The count of words Second\nIs: 1\n"
                }
            ]
        }, 
        {
            "source": "### Get the file from a URL", 
            "metadata": {}, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": 22, 
            "source": "!rm README.md* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2017-09-21 14:42:50--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/README.md\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3624 (3.5K) [text/plain]\nSaving to: \u2018README.md\u2019\n\n100%[======================================>] 3,624       --.-K/s   in 0s      \n\n2017-09-21 14:42:50 (20.1 MB/s) - \u2018README.md\u2019 saved [3624/3624]\n\n"
                }
            ]
        }, 
        {
            "execution_count": 23, 
            "source": "textfile_rdd = sc.textFile(\"README.md\")\ntextfile_rdd.count()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 23, 
                    "data": {
                        "text/plain": "98"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 24, 
            "source": "textfile_rdd.take(5)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 24, 
                    "data": {
                        "text/plain": "[u'# Apache Spark',\n u'',\n u'Spark is a fast and general cluster computing system for Big Data. It provides',\n u'high-level APIs in Scala, Java, and Python, and an optimized engine that',\n u'supports general computation graphs for data analysis. It also supports a']"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 25, 
            "source": "Spark_lines = textfile_rdd.filter(lambda line: \"Spark\" in line)\nSpark_lines.first()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 25, 
                    "data": {
                        "text/plain": "u'# Apache Spark'"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 26, 
            "source": "print \"The file README.md has \" + str(Spark_lines.count()) + \\\n\" of \" + str(textfile_rdd.count()) + \\\n\" Lines with word Spark in it.\"", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "The file README.md has 19 of 98 Lines with word Spark in it.\n"
                }
            ]
        }, 
        {
            "execution_count": 27, 
            "source": "temp = Spark_lines.flatMap(lambda line:line.split(\" \")).map(lambda x:(x,1)).reduceByKey(add)\ntemp.filter(lambda (k,v): k.startswith(\"Spark\")).collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 27, 
                    "data": {
                        "text/plain": "[(u'Spark](#building-spark).', 1),\n (u'Spark', 14),\n (u'SparkPi', 2),\n (u'Spark.', 1),\n (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 28, 
            "source": "temp.filter(lambda (k,v): \"Spark\" in k).collect()", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "execution_count": 28, 
                    "data": {
                        "text/plain": "[(u'Spark](#building-spark).', 1),\n (u'Spark', 14),\n (u'tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n  1),\n (u'SparkPi', 2),\n (u'Spark\"](http://spark.apache.org/docs/latest/building-spark.html).', 1),\n (u'Spark.', 1)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": 29, 
            "source": "!rm Scores.txt* -f\n!wget https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\n \nRaw_Rdd = sc.textFile(\"Scores.txt\")\n\nSumScores = Raw_Rdd.map(lambda l: l.split(\",\")).\\\nmap(lambda v : (v[0], int(v[1])+int(v[2])+int(v[3])+int(v[4])))\n\nFinal = SumScores.map(lambda avg: (avg[0],avg[1],avg[1]/4.0))\n\nFinal.take(5)", 
            "metadata": {}, 
            "cell_type": "code", 
            "outputs": [
                {
                    "name": "stdout", 
                    "output_type": "stream", 
                    "text": "--2017-09-21 14:45:31--  https://raw.githubusercontent.com/carloapp2/SparkPOT/master/Scores.txt\r\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.48.133\r\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.48.133|:443... connected.\r\nHTTP request sent, awaiting response... 200 OK\r\nLength: 75 [text/plain]\r\nSaving to: \u2018Scores.txt\u2019\r\n\r\n100%[======================================>] 75          --.-K/s   in 0s      \r\n\r\n2017-09-21 14:45:31 (13.2 MB/s) - \u2018Scores.txt\u2019 saved [75/75]\r\n\r\n"
                }, 
                {
                    "execution_count": 29, 
                    "data": {
                        "text/plain": "[(u'Carlo', 15, 3.75),\n (u'Mokhtar', 15, 3.75),\n (u'Jacques', 15, 3.75),\n (u'Braden', 15, 3.75),\n (u'Chris', 15, 3.75)]"
                    }, 
                    "metadata": {}, 
                    "output_type": "execute_result"
                }
            ]
        }, 
        {
            "execution_count": null, 
            "source": "", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "code", 
            "outputs": []
        }
    ], 
    "nbformat": 4
}